<!-- 

TODO: Read carpenter & Bithell

Chernick & Labudde (2009)

- Cited references: Carpenter & Bithell (2000), Manly (2006)
- Percentile is the originally proposed bootstrap CI
- Schenker (1985) showed slow convergence of Perc, so Efron devised BCa (1987)
- Hall (1992):
    * BCa, iterated bootstrap, and bootstrap $t$ are second order accurage, meaning they converge to asymptotic confidence level more quickly.
- Table 1 (From Efron & Tibshirani, 1986); asymptotic results only
    * Norm CI: correct if sampling distribution is normal with constant $\sigma^2$ and no bias
    * Perc CI: There exists a monotone transformation such that the transformed distribution is normal with constant $\sigma^2$, with no bias in the transformed distribution
    * BC: Like Perc CI, but can allow constant bias
    * BCa: Like BC, but allow bias proportional to variance
- Simulation
    - Bootstrap procedures in simulation: t-CI (use quantiles in $t$), Perc, BC, BCa, Efron-DiCiccio approximate bootstrap confidence interval.
    - Results:
        * Gamma(2, 3): 84.4% for EP, 88% for ABC, 88.6% for BCa
        * t(df = 5): ~ 85% for N = 400; BCa better than Perc
        * Uniform(0, 1): Norm CI shows better coverage than Perc; especially for smaller samples.
    - Normal(0, 1): BCa best, followed by normal
    - Log(Normal[0, 1]): BCa better, but only 61.66% coverage for small sample size
- For symmetric distribution, Normal-t, Perc, and BC become second-order accurate

Diciccio & Romano (1988)
- "the bootstrap is motivated as a method to estimate the distributions of approximate pivot"
- BCa "depends on the calculation of the acceleration constant."
- "In any given situation, the choice of bootstrap procedure depends on available theoretical results, computational considerations, the level of accuracy desired, simulation results and experience with similar problems." (p. 339)
- Transformation theory:
    * Monotone transformation of $\hat \theta$ have a symmetric sampling distribution centered at the true estimate with constant variance
    * BC: account for bias
    * BCa: account for skewness (acceleration: how the bias "accelerated" as a function of the true value)
        * Also requires choosing the acceleration constant $a$, which is not easy to determine
        * Efron proposed using the influence function (p. 345)
- "The empirical distribution function is an optimal estimator of F in a local asymptotic minimax (LAM) sense"
    * Under suitable conditions, bootstrap CI is LAM
- Bootstrap need not be consistent
- Studentizing: the pivot converges to a standard normal distribution, so it's distribution is less dependent on the true distribution function of the estimator (especially when the variance is a function of the mean, etc)
- Prepivoting: iteratively improve the coverage by determining the coverage of the sample statistic
    * However, iteration could make things worse

Wichmann & Hill (2001)
- 

Lai et al. (2020)
- Studentized CI typically gives the widest interval, although nonparametric BCa is widest in some conditions. 
- Basic norm are better than PERC and BCa

Need to find other studies on CIs for MLM

Falk et al. (2022, preprint)
- Double bootstrap or Bayesian
- Only using percentile interval
- Residual bootstrap CI performs reasonably well.

Text:

-->

Arguably, the most common usage of the bootstrap is to obtain confidence intervals (CIs) for model parameters (e.g., fixed and random effects) and derived quantities that are functions of the parameters (e.g., effect size, indirect effects, ICC). Bootstrap CI is most useful for quantities that have nonnormal sampling distributions in finite samples, such as random-effect variances and ICC. For example, @efron1988 showed that, for correlation coefficients, the percentile CI is a better alternative than the the analytic CI based on normal approximation. In behavioral sciences, bootstrap CI has become the standard for studying mediation and making inferences for indirect effects [e.g., @shrout2002]. For multilevel data, there have been several studies on bootstrap CIs for ICC [e.g., @ukoumunne2003; @braschel2016]. 

While there has not been many systematic studies for CIs with multilevel data, we expect that recommendations based on single-level data would still hold [e.g., @diciccio1988, @carpenter2000]. The choice should depend on whether the sample estimate is expected to be biased, and how much the sampling distribution of the target quantity deviates from normal. As shown by @hall1992, the studentized and BCa are second-order accurate, compared to percentile, normal, and basic CIs, which are first-order accurate. This means that studentized and BCa CIs, in theory, would have coverage level converging to the nominal level (e.g., 95%) faster than the other CIs. The drawback for studentized and BCa CIs, however, is that they relies on additional estimates from the sample that may lead to instability: studentized CI requires an estimate of the sampling variance, and BCa CI requires estimating the acceleration constant. This is partly a reason why studentized CI and BCa were not consistently found to outperform other CIs.

The studentized, normal, and basic CIs all assume that the target sampling distribution is symmetric in some fashion, and would likely perform better when such an assumption holds. When the target quantity is expected to be skewed, BCa and percentile CIs may have an advantage as they don't assume symmetric distributions. That said, studentized, normal, and basic CIs can be improved if there exists a transformation so that the transformed sampling distribution is close to symmetric, such as Fisher's $z$ transformation for correlation and log transformation for ICC. Transformation can also be used to avoid out-of-bound confidence limits for bounded quantities; an example is the use of logit transformation for $R^2$ so that the back-transformed confidence limits are between 0 and 1.

While a 95% CI is valid when its coverage is 95% or above, when considering CIs with similar coverage properties, the one with a shorter length is usually preferred [@efron2003]. For example, @efron1988 gave an example where the percentile CI had much shorter width than the normal-theory based CI.

To our knowledge, there has been limited research evaluating different CIs for multilevel data in finite samples, and earlier studies focused almost exclusively on estimating the ICC. @ukoumounne2003 found that, among several bootstrap methods, the studentized CI using a variance stabilizing transformation gave the best CI coverage for ICC across conditions, especially when the number of clusters is small [see also @braschel2016]. @reiser2017 compared studentized, percentile, and BCa CIs for fixed-effect and random-effect variance estimates, and found the three methods to be adequate and similar for fixed effects, and that BCa CIs did not seem to be better than percentile CIs, especially in small samples. @burch2012 found that normal CIs with a log transformation gave better coverage than BCa for variance components, and both CIs were better than the percentile CIs. @lai2021 showed that with nonnormal data, basic and studentized CIs gave best coverage for standardized mean difference effect size, although studentized CIs tended to be slightly wider than other CIs. More research is needed to compare different bootstrap CIs in the multilevel modeling context.

So based on a very limited literature, we recommend researchers use, when possible, studentized CI for quantities with a relatively symmetric sampling distribution, although all intervals should perform similarly if the sampling distributions are close to normal. For quantities with highly skewed sampling distributions (e.g., variance parameters in small samples) or with bounds (e.g., $R^2$), one can use studentized CIs on the transformed parameter (e.g., logit transformation for $R^2$), or use BCa or percentile CIs if studentized CIs cannot be computed and/or an appropriate transformation is not available.
