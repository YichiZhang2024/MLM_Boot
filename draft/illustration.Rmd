
In this section, we illustrate three bootstrap approaches (i.e., parametric, residual, and case) with the *bootmlm* R package and the wild bootstrap with the *lmeresampler* R package on an empirical example. In particular, we will obtain CIs for the fixed effects, intraclass correlations (ICC), and the variance explained by the fixed effects ($R^2$). <!-- ML: Also mention bias-corrected estimates? -->

The empirical example is an elementary school study by @knuver1993 and @doolaard1999, whose data was shared and used in @snijders2012. This study contains data for 3,758 students (after a list-wise deletion of 258 students), from 211 schools with class sizes ranging between 5 and 36. As shown in @snijders2012 (Chapters 8 and 10), the intelligence variable in the data appeared to violate both assumptions of normality and homoscedasticity. 

<!-- YZ: It'll be helpful to mention which variables violate the homoscedasticity assumptions in snijders2012 [X] WT: good suggestion!-->

We adopt the two-level model in @snijders2012 (Chapter 8), with students nested within schools, to predict language test scores ($\text{lang}_{ij}$) with students' intelligence ($\text{IQ}_{ij}$), gender ($\text{gender}_{ij}$), and family's socio-economic status ($\text{SES}_{ij}$). For the continuous level-1 (student-level) predictors, the standard practice is to disaggregate the between- and within-effects by including the level-2 mean (school-level mean) of each predictor in the model and centering the level-1 predictors by the level-2 means. In this example, $\text{school\_IQ}_j$ is the school mean of intelligence, and $\text{school\_SES}$ is the school mean of socio-economic status. The equation of the full model is <!-- ML: The zeros are not consistent with the ones in the intro -->
\begin{align}
\begin{split}
\text{Level 1: } & \text{lang}_{ij} = \beta_{0j} + \beta_{1j}\text{IQ}_{ij} + \beta_{2j}\text{SES}_{ij} + \beta_{3j} \text{gender}_{ij} + \beta_{4j} \text{IQ}_{ij} \times \text{SES}_{ij} + e_{ij}, \quad \epsilon_{ij} \sim N(0, \sigma^2)\\
\text{Level 2: } & \beta_{0j} = \gamma_{00} + \gamma_{01} \text{school\_IQ}_{j} + \gamma_{02} \text{school\_SES}_{j} + \gamma_{03}\text{school\_IQ}_j\times\text{school\_SES}_j + u_{0j} \\
& \beta_{1j} = \gamma_{10} + u_{1j}\\
& \beta_{2j} = \gamma_{20} \\ 
& \beta_{3j} = \gamma_{30} \\ 
& \beta_{4j} = \gamma_{40} \\
& \begin{pmatrix}
u_{0j} \\ u_{1j}
\end{pmatrix}\sim N\left(
\begin{bmatrix}0 \\ 
0 \end{bmatrix}, 
\begin{bmatrix}\tau^2_{00}\\
\tau_{01} & \tau^2_{11}
\end{bmatrix}\right) \\
\text{Combined: } & \text{lang}_{ij} = \gamma_{00} + \gamma_{10}\text{IQ}_{ij} + \gamma_{20}\text{SES}_{ij} + \gamma_{30}\text{gender}_{ij} + \gamma_{40}\text{IQ}_{ij} \times \text{SES}_{ij}\text{gender}_{ij} + \\
& \quad \gamma_{01} \text{school\_IQ}_{j} + \gamma_{02} \text{school\_SES}_j + \gamma_{03} \text{school\_SES}_j \times \text{school\_IQ}_j + \\
& \quad u_{0j} + u_{1j} \text{IQ}_{ij} + e_{ij}, 
(\#eq:sj-mod)
\end{split}
\end{align}
where $\text{IQ}_j$ and $\text{school\_SES}_j$ are the average intelligence and SES of students within each school, respectively; <!-- ML: I think you've explained school_SES and school_IQ already-->$\gamma_{00}$ is the grand intercept of language scores; $\gamma_{01}$ and $\gamma_{02}$ are the school-level fixed effects of intelligence and SES, respectively; $\gamma_{03}$ is the school-level interaction between intelligence and SES; $\gamma_{10}$ and $\gamma_{20}$ are the student-level fixed effects of intelligence and SES, respectively; $\gamma_{30}$ is the student-level interaction between intelligence and SES; $u_{0j}$ is the random effect from the grand intercept; and $u_{1j}$ is the random effect from the student-level intelligence. For reproducibility, we use the following syntax to download and import the dataset from the website of @snijders2012 to our local R environment. 

<!-- YZ: Maybe add a sentence to explain the difference between $IQ_j$ and $school_IQ_j$? Something like we  [X] WT: added to line 47 -->
\singlespacing

```{r}
temp <- tempfile()
download.file("https://www.stats.ox.ac.uk/~snijders/mlbook2_r_dat.zip", temp)
sj_dat <- read.table(unz(temp, "mlbook2_r.dat"), header = TRUE)
```

\doublespacing

<!-- The `bootmlm` R package is available on github and can be installed using the *remotes* package.  -->

```{r eval=FALSE, echo=FALSE}
# install.packages("remotes")
remotes::install_github("marklhc/bootmlm")
```

<!-- YZ: I think here we can mention the R packages we will use in this example, but we don't need to show how to install and load them? [X] WT: sure -->
\singlespacing

```{r message=FALSE, echo=FALSE}
options(width = 72)  # force line wrap
library(lme4)
library(bootmlm)
library(lmeresampler)
library(boot)
library(MuMIn)
```


\doublespacing

In this illustration, we will use five R packages: *lme4* [version 1.1.30\; @bates2015] for multilevel analysis; *MuMIn* [version 1.47.1\; @barton2022] for computing $R^2$; *bootmlm* [version 0.0.1.1000\; @lai2021] for parametric, residual, and cases bootstrap; *lmeresampler* [version 0.2.2\; @loy2022] for wild bootstrap; and *boot* [version 1.3.28\; @canty2021; @davison1997] for computing bootstrap CIs. We perform two models in *lme4*: (a) the full model in @snijders2012, as shown in Model\ \@ref(eq:sj-mod), and (b) the null model (the unconditional model) that has no predictors, given by
\begin{align}
\begin{split}
\text{lang}_{ij} & = \gamma_{00} + u_{0j} + \epsilon_{ij} \\
u_{0j} & \sim N(0, \tau^2_{00}), \quad \epsilon_{ij} \sim N(0, \sigma^2),
\end{split}
\end{align}
to obtain the ICC: $\rho = \frac{\tau^2_{00}}{\tau^2_{00} + \sigma^2}$. 
<!-- YZ: We need to double check the consistency of symbols and equations once we finish the chapter WT: Sure. Changed to epsilon -->
\singlespacing

```{r}
# unconditional model
mod_0 <- lmer(langPOST ~ (1 | schoolnr), data = sj_dat)
# final model
mod_f <- lmer(langPOST ~ IQ_verb*ses + sex + sch_iqv*sch_ses +
                (IQ_verb | schoolnr), 
              data = sj_dat)
```

\doublespacing

<!-- We compute ICC, defined as $\frac{\tau^2_{00}}{\tau^2_{00} + \sigma^2}$, based on the null model and obtain the fixed effects and variance explained by the fixed effects based on the final model.  -->
We first define two functions: `fixef_rsq()` takes the full model to compute the fixed effects and the variance explained by the fixed effects ($R^2$), and `icc()` takes the null model to compute ICC. `fixef_rsq()` yields estimates for nine parameters, including the eight fixed effects ($\gamma_{00}, \gamma_{01}, \gamma_{02}, \gamma_{03}, \gamma_{10}, \gamma_{20}, \gamma_{30}, \gamma_{40}$) and the variance explained by the fixed effects ($R^2$). In `icc()`, we compute the ICC, defined as $\rho = \frac{\tau^2_{00}}{\tau^2_{00} + \sigma^2}$, and additionally its asymptotic sampling variance, which will be used to construct the bootstrap-*t*/studentized CI. 

\singlespacing

```{r}
fixef_rsq <- function(mod) {
  # Fixed effects
  fix_eff <- fixef(mod)
  # R^2
  rsq <- MuMIn::r.squaredGLMM(mod)[1]
  c(fix_eff = fix_eff, rsq = rsq)
}

icc <- function(mod) {
  # ICC estimate
  vc <- as.data.frame(VarCorr(mod))
  tau00sq <- vc[1, "vcov"]
  sigmasq <- vc[2, "vcov"]
  icc_est <- tau00sq / (tau00sq + sigmasq)
  # ICC variance with delta method
  dG_tau00sq <- sigmasq / (tau00sq + sigmasq)^2
  dG_sigmasq <- - tau00sq / (tau00sq + sigmasq)^2
  grad <- c(dG_tau00sq, dG_sigmasq)
  vcov_ranef <- vcov_vc(mod_0, sd_cor = FALSE)
  icc_var <- t(grad) %*% vcov_ranef %*% grad
  c(icc_est, icc_var)
}
```

\doublespacing

Executing `fixef_rsq(mod_f)` and `icc(mod_0)` gives us the following parameter estimates from the two models. 

\singlespacing

```{r echo=FALSE, warning=FALSE}
out <- c(fixef_rsq(mod_f), icc(mod_0))
names(out) <- c(names(fixef(mod_f)), "Rsq", "ICC", "Var(ICC)")
round(out, 4)
```

\doublespacing

<!-- ## Assumption Checking -->


## Parametric Bootstrap

At the time of writing, the `bootstrap_mer()` function in *bootmlm* supports three main types of bootstrapping---parametric, residual, and case. To perform parametric bootstrap, we denote `type = "parametric"` in the function and specify the fitted model object from *lmer* (i.e., `mod_f` and `mod_0`) along with the defined function for obtaining the parameter estimates (i.e., `fixef_rsq()` and `icc()`). `nsim` refers to the number of bootstrap samples ($R$), which is generally recommended to be large for stable estimates. In this illustration, we use $R = 1,999$. 

\singlespacing

```{r eval=FALSE}
boo_par_fr <- bootstrap_mer(mod_f, fixef_rsq, nsim = 1999L, type = "parametric")
boo_par_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "parametric")
```

\doublespacing

After running bootstrap resampling, we can use the `boot.ci()` function in *boot* to compute the CIs for the bootstrap samples. For the fixed effects and $R^2$ with parametric bootstrap, this function provides three ways to construct CIs: normal, basic, and percentile.<!-- ML: boot.ci() supports five. Would you clarify that studentized and BCa require additional input? --> As `boo_par_fr` consists of the bootstrap samples for nine parameters, `lapply()` allows us to compute CIs for all of them simultaneously. 

\singlespacing

```{r eval=FALSE}
boo_par_fr_ci <- lapply(1:9, function(i) {
  boot.ci(boo_par_fr, type = c("norm", "basic", "perc"), index = i)
})
```

\doublespacing

For ICC, we further demonstrate how to construct the studentized CI. Since `icc()` computes both the estimate and variance of ICC, `boo_par_icc` contains the bootstrap samples of both estimators, i.e., $\hat \rho^*$ and $\text{Var}^*(\hat \rho^*)$. We specify the argument `type = "stud"` in `boot.ci()`, which takes the bootstrap samples $\hat \rho^*$ and $\text{Var}^*(\hat \rho^*)$ to construct the studentized CI. 

```{r eval=FALSE}
boo_par_icc_ci <- boot.ci(boo_par_icc, type = c("norm", "basic", "perc", "stud"))
```


## Wild Bootstrap <!-- ML: Check the order of the 4 bootstrap types with the previous sections -->

While wild bootstrap is unavailable in *bootmlm*, we can use the function `wild_bootstrap()` in the *lmeresampler* R package. Similar to those in `bootstrap_mer()`, the required input arguments in `wild_bootstrap()` are the fitted model object, the defined function that computes the parameter estimates (`.f`), and the number of bootstrap samples (`B`). 

\singlespacing

```{r eval=FALSE}
boo_wil_fr <- wild_bootstrap(mod_f, .f = fixef_rsq, B = 1999L)
boo_wil_icc <- wild_bootstrap(mod_0, .f = icc, B = 1999L)
```

\doublespacing

At the moment of writing this chapter, the bootstrap output from *lmeresampler* (version `r packageVersion("lmeresampler")`) can be summarized with the `confint()` function, which supports three ways of constructing CIs with wild bootstrap: normal, basic, and percentile, with the argument `type = "all"`. 

\singlespacing

```{r eval=FALSE}
boo_wil_fr_ci <- confint(boo_wil_fr, type = "all")
boo_wil_icc_ci <- confint(boo_wil_icc, type = "all")
```

\doublespacing


## Residual Bootstrap

<!-- `bootmlm` implements three methods for residual bootstrap: (a) differentially reflated residual bootstrap, (b) Carpenter-Goldstein-Rashbash's residual bootstrap [CGR; @carpenter2003], and (c) transformational residual bootstrap by @vanderleeden2008.  -->

As the residuals in MLM (denoted as $\tilde u$ and $\tilde \epsilon$) are shrinkage estimates, their sampling variabilities are much smaller than the population random effects (e.g., $\tau^2$ and $\sigma^2$). Residual bootstrap accounts for the small sampling variabilities by rescaling $\tilde u$ and $\tilde \epsilon$. To perform residual bootstrap, we specify `type = "residual"` in `bootmlm::bootstrap_mer`. 
<!-- The transformational residual bootstrap is particular useful for bounded parameters, such as ICC and $R^2$. Here we demonstrate the first two methods and discuss the transformational residual bootstrap in a separate subsection.  -->
<!-- YZ: I only described a general idea of reflating residuals in section III. I'll add some additional details of these three types of reflations WT: I took away the other two residual bootstrap methods, as suggested by Mark -->

<!-- The first residual bootstrap method rescales $\tilde u$ and $\tilde e$ to match their sampling variabilities with those of $u$ and $e$. The second method, CGR, rescales the sample covariance matrix of the realized values of the residuals to match the model-implied variance components. To perform the two residual bootstrap methods, specify `type = "residual"` and `type = "residual_cgr"`, respectively, in `bootmlm::bootstrap_mer`.  -->

\singlespacing

```{r eval=FALSE}
# Reflated residual
boo_res_fr <- bootstrap_mer(mod_f, fixef_rsq, nsim = 1999L, type = "residual")
boo_res_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "residual")
```

\doublespacing

For the fixed effects and $R^2$, we exemplify four ways to construct CIs with `boot::boot.ci()` with residual bootstrap methods: normal, basic, percentile, and additionally, the bias-corrected and accelerated bootstrap (BCa). For ICC, we also illustrate the studentized CI. As discussed in Section III, the influence values are needed to compute CIs with BCa and can be obtained using the function `empinf_mer()` in *botmlm*. This function requires the inputs of the fitted model (`mod_f` and `mod_0`), the defined function that computes the parameter estimate(s) (`fixef_rsq()` and `icc()`), and the index of the statistic in the output of the defined function. As `fixef_rsq()` yields nine statistics, we can use `lapply()` to conveniently compute the influence values for all of them at once. 

\singlespacing

```{r eval=FALSE}
inf_val_fr <- lapply(1:9, function(i) {
  empinf_mer(mod_f, fixef_rsq, index = i)
})
inf_val_icc <- empinf_mer(mod_0, icc, index = 1)
```

We additionally specify `"bca"` in `type` to compute the CI with BCa. 

```{r eval=FALSE}
# Residual bootstrap
boo_res_fr_ci <- lapply(1:9, function(i) {
  boot::boot.ci(boo_res_fr, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_fr[[i]])
})
boo_res_icc_ci <- boot::boot.ci(boo_res_icc, 
                                type = c("norm", "basic", "perc", "stud", "bca"), 
                                L = inf_val_icc)
```

\doublespacing

## Cases Bootstrap

With cases bootstrap in multilevel modeling, clusters are sampled with replacement using the argument of `type = "case"` in `bootmlm::bootstrap_mer()`. Optionally, we can further resample the cases (observations) within each cluster, which can be done using the `lv1_resample = TRUE` argument. Unlike the other bootstrap methods, *bootmlm* currently supports the cases bootstrap for only two-level models. 

\singlespacing

```{r eval=FALSE}
# Fixed effects and Rsq
boo_cas_fr <- bootstrap_mer(mod_f, fixef_rsq, nsim = 1999L, type = "case")
boo_cas1_fr <- bootstrap_mer(mod_f, fixef_rsq, nsim = 1999L, type = "case", 
                             lv1_resample = TRUE)
# ICC
boo_cas_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "case")
boo_cas1_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "case", 
                              lv1_resample = TRUE)
```

\doublespacing

`boot::boot.ci()` also supports five ways to compute CI, including normal, basic, percentile, studentized, and BCa. Again, the influence values are needed to compute CIs with BCa, and we use the `lapply()` function to compute the CIs for all nine statistics defined in `fixef_rsq()` in one command.  

\singlespacing

```{r eval=FALSE}
# Only resampling clusters
boo_cas_fr_ci <- lapply(1:9, function(i) {
  boot::boot.ci(boo_cas_fr, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_fr[[i]])
})
boo_cas_icc_ci <- boot::boot.ci(boo_cas_icc, 
                                type = c("norm", "basic", "perc", "stud", "bca"), 
                                index = 1L, L = inf_val_icc)
# Resampling both clusters and individuals
boo_cas1_fr_ci <- lapply(1:9, function(i) {
  boot::boot.ci(boo_cas1_fr, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_fr)
})
boo_cas1_icc_ci <- boot::boot.ci(boo_cas1_icc, 
                                 type = c("norm", "basic", "perc", "stud", "bca"), 
                                 index = 1L, L = inf_val_icc)
```

\doublespacing

## Bootstrap CI With Transformation

For parameters with highly skewed sampling distributions or with bounds (e.g., $R^2$ and ICC), obtaining the CI from the transformed parameters may give better coverage at the 95% intervals [@ukoumunne2003]. For example, we can transform $R^2$ to an unbounded scale with logit transformation, obtain the CIs on the transformed scale, and back-transform the CIs to the bounded scale between 0 and 1. One can also use the same procedure to transform ICC with the stabilizing transformation suggested by @ukoumunne2003. 

Here we demonstrate how to obtain CIs for $R^2$ with the logit transformation. In `boot::boot.ci()`, we additionally provide three inputs: (a) the quantile function of the logistic distribution, `qlogis`, (b) the function of the first derivative of quantile function `function(x) 1 / (x - x^2)`, and (c) the inverse quantile function (i.e., the probability density function) of the logistic distribution. As $R^2$ is the ninth parameter in our defined function `fixef_rsq()` for bootstrapping, we specify `index = 9` to compute the CI for $R^2$. Below is the syntax for applying logit transformation for the bootstrap samples of $R^2$ with parametric, residual, and cases bootstrap. 

\singlespacing

```{r eval=FALSE}
# Parametric
boo_par_rsq_ci <- boot.ci(boo_par_fr, h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 9, type = c("norm", "basic", "perc"))
# Residual
boo_res_rsq_ci <- boot.ci(boo_res_fr, L = inf_val_fr, h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 9, type = c("norm", "basic", "perc", "bca"))
# Cases (resampling only clusters)
boo_cas_rsq_ci <- boot.ci(boo_cas_fr, L = inf_val_fr, h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 9, type = c("norm", "basic", "perc", "bca"))
# Cases (resampling clusters and individuals)
boo_cas1_rsq_ci <- boot.ci(boo_cas1_fr, L = inf_val_fr, h = qlogis, 
                           hdot = function(x) 1 / (x - x^2), 
                           hinv = plogis, 
                           index = 9, type = c("norm", "basic", "perc", "bca"))
```

\doublespacing


## Comparison

```{r echo=FALSE}
compare_boo <- readRDS("illustration_boo_res/compare_boo.rds")
```

```{r echo=FALSE}
tab1_cap <- "
Bootstrap Standard Errors and Confidence Intervals for School-Level Effects
"
tab2_cap <- "
Bootstrap Standard Errors and Confidence Intervals for Student-Level Effects
"
tab3_cap <- "
Bootstrap Standard Errors and Confidence Intervals for $R^2$ and ICC
"
```


```{r comp1, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, X.Intercept., sch_iqv, sch_ses, sch_iqv.sch_ses) %>%
  filter(!stat == "Studentized", 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Bootstrap Type", "$\\gamma_{00}$", "$\\gamma_{01}$", 
                  "$\\gamma_{02}$", "$\\gamma_{03}$"), 
    caption = tab1_cap
  ) %>%
  collapse_rows(1:2) %>%
  add_header_above(c(" " = 2, "School-Level Fixed Effects" = 4), 
                   escape = FALSE) %>%
  kable_styling(font_size = 10.5)
```

```{r comp2, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, IQ_verb, ses, IQ_verb.ses, sex) %>%
  filter(!stat == "Studentized", 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Bootstrap Type", "$\\gamma_{10}$", 
                  "$\\gamma_{20}$", "$\\gamma_{30}$", "$\\gamma_{40}$"), 
    caption = tab2_cap
  ) %>%
  collapse_rows(1:2) %>%
  add_header_above(c(" " = 2, "Student-Level Fixed Effects" = 4),  
                   escape = FALSE) %>%
  kable_styling(font_size = 10.5)
```

```{r comp3, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, rsq, rsq_trans, icc) %>%
  filter(!(stat %in% c("Studentized", "BCA") & boo_type == "Wild"), 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Bootstrap Type", "$R^2$", "$R^2$ (transformed)", 
                  "ICC"), 
    caption = tab3_cap
  ) %>%
  collapse_rows(1:2) %>%
  kable_styling(font_size = 10.5)
```

Tables\ \@ref(tab:comp1)-\ \@ref(tab:comp3) summarize the standard errors and CIs for all parameters with the illustrated bootstrap approaches. Recall that parametric bootstrap assumes normality; residual bootstrap relaxes the normality assumption but still relies on the homoscedasticity assumption; cases bootstrap relaxes both assumptions but requires more time to perform with lower computational efficiency. Since most variables in the data violate the homoscedasticity assumption, and the intelligence variable further violates the normality assumption, we expect to see differences in the standard errors and CIs between methods, particularly for the fixed effects related to intelligence. 

For the school-level fixed effects, the CIs are similar across methods, except for the school-mean intelligence ($\gamma_{01}$). The standard errors are also similar across methods, except for the grand intercept ($\gamma_{00}$) and the school-mean intelligence. The cases bootstrap with resampling at both levels shows the most drastic differences with larger standard errors and wider CIs. For the student-level fixed effects, the standard errors and CIs are similar across methods, except for the student-level intelligence ($\gamma_{10}$) and the interaction between the student-level intelligence and socio-economic status ($\gamma_{30}$). Similarly, the cases bootstrap with resampling at both levels gives larger standard errors and wider CIs.

For $R^2$, the transformed $R^2$, and ICC, all methods yield similar standard errors and CIs are similar. Cases bootstrap with resampling at both levels tend to give a lower normal and basic CIs than other CI methods for ICC. <!-- ML: you mean "narrower" CI? lower CI doesn't seem to make sense -->







