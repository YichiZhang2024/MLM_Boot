
In this section, we illustrate three bootstrap approaches (i.e., parametric, residual, and cases) with the *bootmlm* R package and the wild bootstrap with the *lmeresampler* R package on an empirical example. In particular, we will obtain bias-corrected estimates, standard errors, and CIs for the fixed effects, random effects, intraclass correlation (ICC), and $R^2$ [@nakagawa2017]. <!-- ML: Also mention bias-corrected estimates? WT: done. Thanks-->

The empirical example is an elementary school study by @knuver1993 and @doolaard1999, whose data was shared and used in @snijders2012. This study contains data for 3,758 students (after a list-wise deletion of 258 students), from 211 schools with class sizes ranging between 5 and 36. As shown in Snijders and Bosker (2012; Chapters 8 and 10), the intelligence variable in the data appeared to violate both assumptions of normality and homoscedasticity. 

<!-- YZ: It'll be helpful to mention which variables violate the homoscedasticity assumptions in snijders2012 [X] WT: good suggestion!-->
<!-- YZ: I made some modifications to the equation below, see if it makes sense-->
We adopt the two-level model in @snijders2012, with students nested within schools, to predict language test scores ($\text{lang}_{ij}$) with students' intelligence ($\text{IQ}_{ij}$), gender ($\text{gender}_{ij}$), and family's socio-economic status ($\text{SES}_{ij}$). For the continuous level-1 (student-level) predictors, the standard practice is to disaggregate the between- and within-effects by including the level-2 mean (school-level mean) of each predictor in the model and centering the level-1 predictors by the level-2 means. In this example, $\text{school\_IQ}_j$ is the school mean of intelligence, and $\text{school\_SES}_j$ is the school mean of socio-economic status. The equation of the full model is <!-- ML: The zeros are not consistent with the ones in the intro -->
\begin{align}
\begin{split}
\text{Level 1: } & \text{lang}_{ij} = \beta_{0j} + \beta_{1j}\text{IQ}_{ij} + \beta_{2j}\text{SES}_{ij} + \beta_{3j} \text{gender}_{ij} + \beta_{4j} \text{IQ}_{ij} \times \text{SES}_{ij} + \epsilon_{ij}, \quad \epsilon_{ij} \sim N(0, \sigma^2)\\
\text{Level 2: } & \beta_{0j} = \gamma_{00} + \gamma_{01} \text{school\_IQ}_{j} + \gamma_{02} \text{school\_SES}_{j} + \gamma_{03}\text{school\_IQ}_j\times\text{school\_SES}_j + u_{0j} \\
& \beta_{1j} = \gamma_{10} + u_{1j}\\
& \beta_{2j} = \gamma_{20} \\ 
& \beta_{3j} = \gamma_{30} \\ 
& \beta_{4j} = \gamma_{40} \\
& \begin{pmatrix}
u_{0j} \\ u_{1j}
\end{pmatrix}\sim N\left(
\begin{bmatrix}0 \\ 
0 \end{bmatrix}, 
\begin{bmatrix}\tau^2_{00}\\
\tau_{01} & \tau^2_{11}
\end{bmatrix}\right) \\
\text{Combined: } & \text{lang}_{ij} = \gamma_{00} + \gamma_{10}\text{IQ}_{ij} + \gamma_{20}\text{SES}_{ij} + \gamma_{30}\text{gender}_{ij} + \gamma_{40}\text{IQ}_{ij} \times \text{SES}_{ij} + \\
& \quad \gamma_{01} \text{school\_IQ}_{j} + \gamma_{02} \text{school\_SES}_j + \gamma_{03} \text{school\_IQ}_j \times \text{school\_SES}_j + \\
& \quad u_{0j} + u_{1j} \text{IQ}_{ij} + \epsilon_{ij}, 
(\#eq:sj-mod)
\end{split}
\end{align}
where <!-- ML: I think you've explained school_SES and school_IQ already. WT: good catch, thanks!-->$\gamma_{00}$ is the grand intercept of language scores; $\gamma_{01}$ and $\gamma_{02}$ are the school-level fixed effects of intelligence and SES, respectively; $\gamma_{03}$ is the school-level interaction between intelligence and SES; $\gamma_{10}$ and $\gamma_{20}$ are the student-level fixed effects of intelligence and SES, respectively; $\gamma_{30}$ is the student-level interaction between intelligence and SES; $u_{0j}$ is the random effect from the grand intercept; and $u_{1j}$ is the random effect from the student-level intelligence. For reproducibility, we use the following syntax to download and import the dataset from the website of @snijders2012 to our local R environment. 

<!-- YZ: Maybe add a sentence to explain the difference between $IQ_j$ and $school_IQ_j$? Something like we  [X] WT: added to line 47 -->
\singlespacing

```{r}
temp <- tempfile()
download.file("https://www.stats.ox.ac.uk/~snijders/mlbook2_r_dat.zip", temp)
sj_dat <- read.table(unz(temp, "mlbook2_r.dat"), header = TRUE)
```

\doublespacing

```{r eval=FALSE, echo=FALSE}
# install.packages("remotes")
remotes::install_github("marklhc/bootmlm")
```

<!-- YZ: I think here we can mention the R packages we will use in this example, but we don't need to show how to install and load them? [X] WT: sure -->
\singlespacing

```{r message=FALSE, echo=FALSE}
options(width = 72)  # force line wrap
library(lme4)
library(bootmlm)
library(lmeresampler)
library(boot)
library(performance)
```

\doublespacing
<!-- YZ: This paragraph is very clear. -->
In this illustration, we will use five R packages: *lme4* [version 1.1.30\; @bates2015] for multilevel analysis; *performance* [version 0.10.2\; @barton2022] for computing $R^2$; *bootmlm* [version 0.0.1.1000\; @lai2021] for parametric, residual, and cases bootstrap; *lmeresampler* [version 0.2.2\; @loy2022] for wild bootstrap; and *boot* [version 1.3.28\; @canty2021; @davison1997] for computing bootstrap CIs. We perform two models in *lme4*: (a) the full model in @snijders2012, as shown in Model\ \@ref(eq:sj-mod), and (b) the null model (the unconditional model) that has no predictors, given by
\begin{align}
\begin{split}
\text{lang}_{ij} & = \gamma_{00} + u_{0j} + \epsilon_{ij} \\
u_{0j} & \sim N(0, \tau^2_{00|U}), \quad \epsilon_{ij} \sim N(0, \sigma^2_{|U}),
\end{split}
\end{align}
to obtain the ICC: $\rho = \frac{\tau^2_{00|U}}{\tau^2_{00|U} + \sigma^2_{|U}}$. 
<!-- YZ: We need to double check the consistency of symbols and equations once we finish the chapter WT: Sure. Changed to epsilon -->
\singlespacing

```{r}
# unconditional model
mod_0 <- lmer(langPOST ~ (1 | schoolnr), data = sj_dat)
# final model
mod_f <- lmer(langPOST ~ IQ_verb*ses + sex + sch_iqv*sch_ses +
                (IQ_verb | schoolnr), 
              data = sj_dat)
```

\doublespacing

We first define two functions: `eff_rsq()` takes the full model to compute the fixed effects and $R^2$, and `icc()` takes the null model to compute ICC. `eff_rsq()` yields estimates for 13 parameters, including eight fixed effects ($\gamma_{00}, \gamma_{01}, \gamma_{02}, \gamma_{03}, \gamma_{10}, \gamma_{20}, \gamma_{30}, \gamma_{40}$), four random effects ($\tau^2_{00}, \tau^2_{11}, \tau_{01}, \sigma^2$), and $R^2$. In `icc()`, we compute the ICC and additionally its asymptotic sampling variance using the `vcov_vc()` function from *bootmlm*, which will be needed to construct the studentized/bootstrap-*t* CI. 

\singlespacing

```{r}
eff_rsq <- function(mod) {
  # Fixed effects
  fix_eff <- fixef(mod)
  # Random effects
  ran_eff <- as.data.frame(VarCorr(mod))$vcov
  # R^2
  # Alternatively, we can get the same R^2 with MuMIn::r.squaredGLMM()
  rsq <- performance::r2_nakagawa(mod)[[1]]
  c(fix_eff = fix_eff, ran_eff = ran_eff, rsq = rsq)
}

icc <- function(mod) {
  # ICC estimate
  vc <- as.data.frame(VarCorr(mod))
  tau00sq <- vc[1, "vcov"]
  sigmasq <- vc[2, "vcov"]
  icc_est <- tau00sq / (tau00sq + sigmasq)
  # ICC variance with delta method
  dG_tau00sq <- sigmasq / (tau00sq + sigmasq)^2
  dG_sigmasq <- - tau00sq / (tau00sq + sigmasq)^2
  grad <- c(dG_tau00sq, dG_sigmasq)
  vcov_ranef <- vcov_vc(mod, sd_cor = FALSE)
  icc_var <- t(grad) %*% vcov_ranef %*% grad
  c(icc_est, icc_var)
}
```

\doublespacing

Executing `eff_rsq(mod_f)` and `icc(mod_0)` gives us the following parameter estimates from the two models. 

\singlespacing

```{r echo=FALSE, warning=FALSE}
out <- c(eff_rsq(mod_f), icc(mod_0))
names(out) <- c(names(fixef(mod_f)), "tau00sq", "tau11sq", 
                "tau01", "sigmasq", "Rsq", "ICC", "Var(ICC)")
round(out, 4)
```

\doublespacing

## Parametric Bootstrap

At the time of writing, the `bootstrap_mer()` function in *bootmlm* supports three main types of bootstrapping---parametric, residual, and cases. To perform parametric bootstrap, we denote `type = "parametric"` in the function and specify the fitted model object from *lmer* (i.e., `mod_f` and `mod_0`) along with the defined function for obtaining the parameter estimates (i.e., `eff_rsq()` and `icc()`). The `nsim` option refers to the number of bootstrap samples ($B$), which is generally recommended to be large for stable estimates. In this illustration, we use $B = 1,999$ because existing literature suggests $.95(B + 1)$ should be an integer when computing the $95\%$ interval for an exact test [@davidson2000; @wilcox2010]. 

\singlespacing

```{r eval=FALSE}
boo_par_eff <- bootstrap_mer(mod_f, eff_rsq, nsim = 1999L, type = "parametric")
boo_par_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "parametric")
```

\doublespacing

After running bootstrap resampling, we can use the `boot.ci()` function in *boot* to compute the CIs for the bootstrap samples. This function provides five ways to construct CIs: normal, basic, percentile, studentized, and BCa. Computing studentized and BCa CIs require additional inputs, such as the influence values and the bootstrap samples of an estimator's variance. With parametric bootstrap, we demonstrate how to obtain normal, basic, and percentile CIs for the fixed effects and $R^2$. In `boot.ci()`, we specify the types of CIs `type = c("norm", "basic", "perc")` to be computed. The index denotes the position of the estimator of interest in the output from `bootstrap_mer()`. 
<!-- ML: boot.ci() supports five. Would you clarify that studentized and BCa require additional input? WT: sounds good. --> As `boo_par_eff` consists of the bootstrap samples for 13 parameters, `lapply()` allows us to compute CIs for all of them simultaneously. 

\singlespacing

```{r eval=FALSE}
boo_par_eff_ci <- lapply(1:13, function(i) {
  boot.ci(boo_par_eff, type = c("norm", "basic", "perc"), index = i)
})
```

\doublespacing

For the ICC, we further demonstrate how to construct the studentized CI. Since `icc()` computes both the estimate and asymptotic sampling variance of ICC, `boo_par_icc` contains the bootstrap samples of both estimators, i.e., $\hat \rho^*$ and $\hat{\text{Var}}(\hat \rho^*)$. We specify the argument `type = "stud"` in `boot.ci()`, which takes the bootstrap samples of $\hat \rho^*$ and $\hat{\text{Var}}(\hat \rho^*)$ to construct the studentized CI. 

```{r eval=FALSE}
boo_par_icc_ci <- boot.ci(boo_par_icc, type = c("norm", "basic", "perc", "stud"))
```


## Residual Bootstrap

As the residuals in MLM (denoted as $\tilde u$ and $\tilde \epsilon$) are shrinkage estimates, their sampling variabilities are much smaller than the population random effects (e.g., $\tau^2_{00}$ and $\sigma^2$). Residual bootstrap accounts for the small sampling variabilities by rescaling $\tilde u$ and $\tilde \epsilon$. To perform residual bootstrap, we specify `type = "residual"` in `bootmlm::bootstrap_mer()`. 

<!-- YZ: I only described a general idea of reflating residuals in section III. I'll add some additional details of these three types of reflations WT: I took away the other two residual bootstrap methods, as suggested by Mark -->

\singlespacing

```{r eval=FALSE}
# Reflated residual
boo_res_eff <- bootstrap_mer(mod_f, eff_rsq, nsim = 1999L, type = "residual")
boo_res_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "residual")
```

\doublespacing

For the fixed effects and $R^2$, we exemplify four ways to construct CIs with `boot::boot.ci()` with residual bootstrap methods: normal, basic, percentile, and additionally, BCa. For the ICC, we also illustrate the studentized CI. As discussed in Section III, the influence values are needed to compute CIs with BCa and can be obtained using the function `empinf_mer()` in *bootmlm*. This function requires the inputs of the fitted model (`mod_f` and `mod_0`), the defined function that computes the parameter estimate(s) (`eff_rsq()` and `icc()`), and the index of the statistic in the output of the defined function. As `eff_rsq()` yields 13 statistics, we can use `lapply()` to conveniently compute the influence values for all of them at once. 

\singlespacing

```{r eval=FALSE}
inf_val_eff <- lapply(1:13, function(i) {
  empinf_mer(mod_f, eff_rsq, index = i)
})
inf_val_icc <- empinf_mer(mod_0, icc, index = 1)
```

We additionally specify `"bca"` in `type` to compute the CI with BCa. 

```{r eval=FALSE}
# Residual bootstrap
boo_res_eff_ci <- lapply(1:13, function(i) {
  boot::boot.ci(boo_res_eff, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_eff[[i]])
})
boo_res_icc_ci <- boot::boot.ci(boo_res_icc, 
                                type = c("norm", "basic", "perc", "stud", "bca"), 
                                L = inf_val_icc)
```

\doublespacing

## Wild Bootstrap <!-- ML: Check the order of the 4 bootstrap types with the previous sections -->

While wild bootstrap is unavailable in *bootmlm*, we can use the function `wild_bootstrap()` in the *lmeresampler* R package. Similar to those in `bootstrap_mer()`, the required input arguments in `wild_bootstrap()` are the fitted model object, the defined function that computes the parameter estimates (`.f`), and the number of bootstrap samples (`B`). 

\singlespacing

```{r eval=FALSE}
boo_wil_eff <- wild_bootstrap(mod_f, .f = eff_rsq, B = 1999L)
boo_wil_icc <- wild_bootstrap(mod_0, .f = icc, B = 1999L)
```

\doublespacing

At the moment of writing this chapter, the bootstrap output from *lmeresampler* (version `r packageVersion("lmeresampler")`) can be summarized with the `confint()` function, which supports three ways of constructing CIs with wild bootstrap: normal, basic, and percentile, with the argument `type = "all"`. 

\singlespacing

```{r eval=FALSE}
boo_wil_eff_ci <- confint(boo_wil_eff, type = "all")
boo_wil_icc_ci <- confint(boo_wil_icc, type = "all")
```

\doublespacing


## Cases Bootstrap

With cases bootstrap in multilevel modeling, clusters are sampled with replacement using the argument of `type = "case"` in `bootmlm::bootstrap_mer()`. Optionally, we can further resample the cases (observations) within each cluster, which can be done using the `lv1_resample = TRUE` argument. Unlike the other bootstrap methods, *bootmlm* currently supports the cases bootstrap for only two-level models. 

\singlespacing

```{r eval=FALSE}
# Fixed effects and Rsq
boo_cas_eff <- bootstrap_mer(mod_f, eff_rsq, nsim = 1999L, type = "case")
boo_cas1_eff <- bootstrap_mer(mod_f, eff_rsq, nsim = 1999L, type = "case", 
                              lv1_resample = TRUE)
# ICC
boo_cas_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "case")
boo_cas1_icc <- bootstrap_mer(mod_0, icc, nsim = 1999L, type = "case", 
                              lv1_resample = TRUE)
```

\doublespacing

The `boot::boot.ci()` function also supports five ways to compute CI, including normal, basic, percentile, studentized, and BCa. Again, the influence values are needed to compute CIs with BCa, and we use the `lapply()` function to compute the CIs for all 13 statistics defined in `eff_rsq()` in one command.  

\singlespacing

```{r eval=FALSE}
# Only resampling clusters
boo_cas_eff_ci <- lapply(1:13, function(i) {
  boot::boot.ci(boo_cas_eff, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_eff[[i]])
})
boo_cas_icc_ci <- boot::boot.ci(boo_cas_icc, 
                                type = c("norm", "basic", "perc", "stud", "bca"), 
                                index = 1L, L = inf_val_icc)
# Resampling both clusters and individuals
boo_cas1_eff_ci <- lapply(1:13, function(i) {
  boot::boot.ci(boo_cas1_eff, type = c("norm", "basic", "perc", "bca"), 
                index = i, L = inf_val_eff[[i]])
})
boo_cas1_icc_ci <- boot::boot.ci(boo_cas1_icc, 
                                 type = c("norm", "basic", "perc", "stud", "bca"), 
                                 index = 1L, L = inf_val_icc)
```

\doublespacing

## Comparison

```{r echo=FALSE}
compare_boo <- readRDS("illustration_boo_res/compare_boo.rds")
```

```{r echo=FALSE}
tab1_cap <- "
Bias-Corrected Estimates, Standard Errors, and Confidence Intervals for School-Level Fixed Effects
"
tab2_cap <- "
Bias-Corrected Estimates, Standard Errors, and Confidence Intervals for Student-Level Fixed Effects
"
tab3_cap <- "
Bias-Corrected Estimates, Standard Errors, and Confidence Intervals for Random Effects
"
tab4_cap <- "
Bias-Corrected Estimates, Standard Errors, and Confidence Intervals for $R^2$ and ICC
"
```


```{r comp1, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, X.Intercept., sch_iqv, sch_ses, sch_iqv.sch_ses) %>%
  filter(!stat == "Studentized", 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Type", "$\\gamma_{00}$", "$\\gamma_{01}$", 
                  "$\\gamma_{02}$", "$\\gamma_{03}$"), 
    caption = tab1_cap
  ) %>%
  collapse_rows(1:2) %>%
  add_header_above(c(" " = 2, "School-Level Fixed Effects" = 4), 
                   escape = FALSE) %>%
  kable_styling(font_size = 10.5)
```

```{r comp2, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, IQ_verb, ses, IQ_verb.ses, sex) %>%
  filter(!stat == "Studentized", 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Type", "$\\gamma_{10}$", 
                  "$\\gamma_{20}$", "$\\gamma_{30}$", "$\\gamma_{40}$"), 
    caption = tab2_cap
  ) %>%
  collapse_rows(1:2) %>%
  add_header_above(c(" " = 2, "Student-Level Fixed Effects" = 4),  
                   escape = FALSE) %>%
  kable_styling(font_size = 10.5)
```

```{r comp3, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, tau00sq:sigmasq) %>%
  filter(!(stat %in% c("SE", "Studentized")), 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Type", "$\\tau^2_{00}$", 
                  "$\\tau^2_{11}$", "$\\tau_{01}$", "$\\sigma^2$"), 
    caption = tab3_cap
  ) %>%
  collapse_rows(1:2) %>%
  add_header_above(c(" " = 2, "Random Effects" = 4),  
                   escape = FALSE) %>%
  kable_styling(font_size = 10.5)
```

```{r comp4, echo=FALSE}
compare_boo %>%
  select(stat:boo_type, rsq, rsq_trans, icc) %>%
  filter(!(stat %in% c("Asymptotic SE", "Likelihood-Based C.I.")), 
         !(stat %in% c("Studentized", "BCA") & boo_type == "Wild"), 
         !(stat == "BCA" & boo_type %in% c("Parametric", "Wild"))) %>%
  knitr::kable(
    booktabs = TRUE, 
    escape = FALSE, 
    col.names = c("", "Type", "$R^2$", "$R^2$ (transformed)", 
                  "ICC"), 
    caption = tab4_cap
  ) %>%
  collapse_rows(1:2) %>%
  kable_styling(font_size = 10.5)
```

Tables\ \@ref(tab:comp1)-\@ref(tab:comp4) summarize the standard errors and CIs for all parameters with the illustrated bootstrap approaches. The tables also include the uncorrected estimates and asymptotic standard errors from the *lme4* model output, as well as the confidence intervals based on profile likelihood using `confint(profile(mod_f, prof.scale = "varcov"))`. Note that with non-bootstrap methods, it is generally difficult to obtain the standard errors and confidence intervals for ICC and $R^2$. Moreover, as there are no direct ways to obtain standard errors of the variance components for the random effects from *lme4* at the time of writing, we relied on the `vcov_vc()` function from *bootmlm* to compute these standard errors. Recall that parametric bootstrap assumes normality; residual bootstrap relaxes the normality assumption but still relies on the homoscedasticity assumption; wild bootstrap relaxes the homoscedasticity assumption but relies on the normality assumption; cases bootstrap relaxes both assumptions but requires more time to perform with lower computational efficiency. Since the intelligence variable violates both the normality and homoscedasticity assumptions, we expect to see differences in the standard errors and CIs between methods for the fixed effects related to intelligence. 

For the school-level fixed effects, the standard errors and CIs are similar across methods, except for the school-mean intelligence ($\gamma_{01}$). The bootstrap CIs are generally narrower than the likelihood-based CI for $\gamma_{01}$. For the student-level fixed effects, the standard errors and CIs are similar across methods, except for the fixed effect of student-level intelligence ($\gamma_{10}$) and the interaction between the student-level intelligence and socio-economic status ($\gamma_{40}$). Similarly, the cases bootstrap with resampling at both levels gives larger standard errors and wider CIs. For school-mean intelligence, student-level intelligence, and the interaction between student-level intelligence and SES ($\gamma_{01}, \gamma_{10}, \gamma_{40}$), the cases bootstrap with resampling at both levels shows the most drastic differences with larger standard errors and wider CIs. 

For the between-variances and covariance between the within- and between-random effects ($\tau^2_{11}, \tau_{01}$), the standard errors and CIs are similar across methods; for the within-variances and residual variances ($\tau^2_{00}, \sigma^2$), however, the standard errors and bootstrap CIs slightly differ across methods. The residual bootstraps yield the smallest SE and narrowest CIs for $\tau^2_{00}$ but a larger SE and wider CIs for $\sigma^2$. The wild bootstraps have a larger SE and wider CIs than all methods, except cases bootstrap with resampling at both levels, for $\tau^2_{00}$ and $\sigma^2$. The cases bootstraps produce unusually small bias-corrected estimates, large SEs, and wide CIs, comparing to other methods. While the cases bootstrap has also been found to perform poorly for variance component estimation [@vanderleeden1997; @vanderleeden2008], we do not recommend cases bootstrap at both levels for the random effect variance components. More studies are needed to evaluate the performance of the cases bootstrap. For the $R^2$ and ICC, all methods yield similar standard errors and CIs, but for ICC in particular, the cases bootstrap at both levels tend to give narrowest intervals with the normal and basic CIs. <!-- ML: you mean "narrower" CI? lower CI doesn't seem to make sense WT: sounds good, thanks-->


## Bootstrap CI With Transformation

For parameters with highly skewed sampling distributions or with bounds (e.g., $R^2$ and ICC), obtaining the CI from the transformed parameters may give better coverage at the 95% intervals [@ukoumunne2003]. For example, we can transform $R^2$ to an unbounded scale with logit transformation, obtain the CIs on the transformed scale, and back-transform the CIs to the bounded scale between 0 and 1. One can also use the same procedure to transform ICC with the stabilizing transformation suggested by @ukoumunne2003. 

Here we demonstrate how to obtain CIs for $R^2$ with the logit transformation ($f[x]$ = $x / [1 - x]$). In `boot::boot.ci()`, we additionally provide three inputs: (a) the logit function, `qlogis`, (b) the first derivative of the logit function function `function(x) 1 / (x - x^2)`, and (c) the inverse of the logit function, `plogis`. As $R^2$ is the 13th parameter in our defined function `eff_rsq()` for bootstrapping, we specify `index = 13` to compute the CI for $R^2$. Below is the syntax for applying logit transformation for the bootstrap samples of $R^2$ with parametric, residual, and cases bootstrap. 

\singlespacing

```{r eval=FALSE}
# Parametric
boo_par_rsq_ci <- boot.ci(boo_par_eff, h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 13, type = c("norm", "basic", "perc"))
# Residual
boo_res_rsq_ci <- boot.ci(boo_res_eff, L = inf_val_fr[[13]], h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 13, type = c("norm", "basic", "perc", "bca"))
# Cases (resampling only clusters)
boo_cas_rsq_ci <- boot.ci(boo_cas_eff, L = inf_val_fr[[13]], h = qlogis, 
                          hdot = function(x) 1 / (x - x^2), 
                          hinv = plogis, 
                          index = 13, type = c("norm", "basic", "perc", "bca"))
# Cases (resampling clusters and individuals)
boo_cas1_rsq_ci <- boot.ci(boo_cas1_eff, L = inf_val_fr[[13]], h = qlogis, 
                           hdot = function(x) 1 / (x - x^2), 
                           hinv = plogis, 
                           index = 13, type = c("norm", "basic", "perc", "bca"))
```

\doublespacing

In this example, the CIs for $R^2$ with transformation are similar to the CIs without transformation, as shown in Table\ \@ref(tab:comp3). 






