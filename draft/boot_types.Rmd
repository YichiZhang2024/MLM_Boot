This section describes the procedures of parametric bootstrap, residual bootstrap, wild bootstrap, and cases bootstrap in detail. A two-level random-effect model is used to illustrate these different types of bootstrapping algorithms. 
<!-- ML: I didn't describe wild bootstrap in my section. Would you say the wild bootstrap is under residuals bootstrap? YZ: I think it's better to leave wild bootstrap as a separate section?-->

## Parametric Bootstrap 

The parametric bootstrap has the strongest assumption among all methods, as it assumes a correct function form and residual distributions [@vanderleeden2008]. Specifically, the level-1 residuals are assumed to follow a normal distribution, $\bv{\epsilon}_j \sim N(\bv{0}, \sigma^2\bv{I}_{n_j})$, and the level-2 residuals are assumed to follow a multivariate normal distribution, $\bv {U}_j \sim N(\bv{0}, \bv{T})$. Let $\bv{\hat \beta}$ be the model estimates from the linear mixed-effects model. The procedure for the parametric bootstrap is described below.
<!-- ML: bivariate or multivariate normal? and I saw Winnie used e while Yichi used epsilon. Also, use bold symbols to be consistent with Winnie's. YZ: sounds good-->

1. Fit a two-level model to data and obtain estimates for the fixed effects and random effects.
2. Draw $J$ sets of level-2 residuals $\bv{U}_j^*$ from a multivariate normal distribution.
3. Draw $J$ sets of level-1 residuals $\bv{\epsilon}_j^*$ of size $n_j$ from a normal distribution.
4. Generate the bootstrap samples $\bv{y}_j^*$, with $\bv{y}_j^* = \bv{X}_j \bv{\hat \beta} + \bv{Z}_j\bv{U}_j^* + \bv{\epsilon}_j^*$. 
5. Refit the two-level model to the bootstrap samples and obtain parameter estimates.
6. Repeat steps 2-5 $B$ times to obtain a bootstrap distribution of parameter estimates. 

## Residual Bootstrap 

Unlike the parametric bootstrap, the residual bootstrap does not assume that the residuals follow certain distributions. However, the homoscedasticity of residuals assumption still needs to hold [@vanderleeden2008]. The procedure for the residual bootstrap is similar to that of the parametric bootstrap, except the residuals are not drawn from assumed distributions (e.g., normal). One issue is that the empirical residuals, extracted from a fitted model, have smaller variances than the population variances [@goldstein2011]. Thus, these residuals are shrinkage estimates and will lead to biased results if used directly to form new bootstrapped samples. To overcome this problem, the residual bootstrap first reflates these shrunken residuals so that their model-implied variances match the population variance [@davison1997; @carpenter2003], then draws residuals from these restored shrunken estimates with replacements [@carpenter2003]. For level-2 residuals $\bv{\hat U}_j$, the model-implied covariance matrix of the estimated residuals at level-2 is denoted as $\bv{T}_{\hat U}$. We transform $\bv{\hat U}_j$ to $\bv{L} \bv{L}^{-1}_{\hat U} \bv{\hat U}$, where $\bv{L}_{\hat U}$ is the lower triangular matrix for the Cholesky decomposition of $\bv{T}_{\hat U}$ such that $\bv{L}_{\hat U} \bv{L}^\top_{\hat U} = \bv{T}_{\hat U}$ and $\bv{L}$ is the Cholesky decomposition of $\bv{T}$. The level-1 residuals can be reflated similarly [@lai2021]. The procedure for the residual bootstrap is as follows:
<!-- Assume the corresponding model estimated covariance matrix of the random coefficients at level-2 is $R$, then the reflated level-2 residuals can be computed by a transformation $\hat U^* = \hat UA$, such that $\frac{\hat U^{*T}\hat U^*}{J} = R$ [@goldstein2011].  -->
<!-- ML: What you described above is from Carpenter et al., which is the residual_cgr procedure. The "residual" procedure in bootmlm calculates the model-implied (not sample) covariance matrix of $\hat u$ (of length $J \times r$), denoted as $T_\hat{u}$, and then transform $\hat U$ to $L L^{-1}_\hat{u} \hat u$, where $L_\hat{u}$ is the lower triangular matrix for the Cholesky decomposition of $T_\hat{u}$ such that $L_\hat{u} L^\top_\hat{u} = T_\hat{u}$ and $L$ is the Cholesky decomposition of $T$. -->

1. Fit a two-level model to data and obtain the estimates for fixed effects and random effects.
2. Reflate the centered empirical residuals using the procedure described above.
3. Draw $J$ sets of level-2 residuals $\bv{U}_j^*$ from the reflated empirical residuals with replacement.
4. Draw $J$ sets of level-1 residuals $\bv{\epsilon}_{j}^*$ of size $n_j$ from the reflated empirical residuals with replacement.
5. Generate the bootstrap samples, $\bv{y}^*_j$, with $\bv{y}_{j}^* = \bv{X}_{j} \bv{\hat \beta} + \bv{Z}_{j} \bv{U}_j^* + \bv{\epsilon}_{j}^*$.
6. Refit the two-level model to the bootstrap data and obtain parameter estimates.
7. Repeat steps 3-6 $B$ times to obtain a bootstrap distribution of parameter estimates. 

Note here we focus on one type of reflating process. There are alternative procedures, such as the procedure described in @carpenter2003 and the transformational residual bootstrap by @vanderleeden2008.

## Wild Bootstrap
<!-- It was first proposed by @liu1988 for ordinary regression models to account for heterscedastic error terms, following the suggestion by @wu1986 [@mackinnon2006]. @flachaire2005 and @davidson2008 later refined the method and provided support for its performance. Recently, @modugno2015 extended the wild bootstrap to multilevel models. -->

The wild bootstrap is a resampling algorithm developed for models with heteroscedastic errors [@wu1986; @modugno2015]. The central idea of the wild bootstrap is to replace the heteroscedastic residuals $\bv{\hat v}_j = \bv{y}_j -  \bv{X}_j \bv{\hat \beta}$ with the heteroscedasticity consistent (HC) covariance matrix estimator proposed by @mackinnon1985, and multiply the residuals with a random variable $m$ drawn from an auxiliary distribution with mean zero and unit variance. In other words, instead of resampling the residuals directly, the wild bootstrap transforms the residuals and draws bootstrap errors from auxiliary distributions. Then the bootstrap responses are generated from this joint distribution, $m\bv v$, and used to compute the statistic of interest [@liu1988]. The commonly used covariance matrix estimators are $\text{HC}_2$ and $\text{HC}_3$, which have been shown to outperform other HC covariance matrix estimators in power [@mackinnon1985; @davidson1993]. Assume $\bv{H}_j = \bv{X}_j(\bv{X}^T\bv{X})^{-1}\bv{X}_j^T$ is the $j$th diagonal block of the orthogonal project matrix:
\begin{equation}
\text{HC}_2: \bv{\tilde v}_j = \text{diag}(\bv{I}_{n_j} - \bv{H}_j)^{-1/2} \circ \bv{\hat v}_j 
\end{equation}
\begin{equation}
\text{HC}_3: \bv{\tilde v}_j = \text{diag}(\bv{I}_{n_j} - \bv{H}_j) \circ \bv{\hat v}_j,
\end{equation}
where $\circ$ is the entry-wise product [@modugno2015].[^hat] The two commonly used auxiliary distributions are $F_1$, proposed by @mammen1993, and $F_2$ (also known as the Rademacher distribution), proposed by @liu1988, where 
\begin{equation}
  F_1: m =
  \begin{cases}
    -(\sqrt{5} -1) /2 \ \ \text{with probability} & p = (\sqrt{5} + 1)/2 \sqrt{5})\\
    (\sqrt{5} + 1) /2  \ \ \text{with probability} & p = 1 - p
  \end{cases}
\end{equation}
and
\begin{equation}
  F_2: m =
  \begin{cases}
    1 \ \ \text{with probability} & p = \frac{1}{2}\\
    -1 \ \ \text{with probability} & p = \frac{1}{2}.
  \end{cases}
\end{equation}

[^hat]: At the time of writing, the *lmeresampler* R package is the only R package that implements the wild bootstrap for MLM, which uses the ordinary least square (OLS) estimators but not generalized least square (GLS). Thus, the hat matrix, $H_j$, does not include the matrix that represents random effects.

The specific procedure for multilevel wild bootstrap is listed below [see @modugno2015, p.4817-4818]:

1. Draw a random sample $m_j$ with $J$ values from the auxiliary distribution that has a mean zero and unit variance such as $F_1$ and $F_2$.
2. Compute the marginal residuals $\bv{\hat v}_j = \bv{y}_j - \bv{X}_j \bv{\hat \beta}$ and transform the residuals to be heteroscedastic consistent $\bv{\tilde v}_j$.
3. Generate the bootstrap samples $\bv{y}_j^* = \bv{X}_j \bv {\hat \beta} + \bv {\tilde v}_j m_j.$
4. Refit the two-level model to the bootstrap data and obtain parameter estimates.
5. Repeat steps 1-4 $B$ times to obtain a bootstrap distribution of bias-corrected parameter estimates.  

The wild bootstrap is expected to perform better than the other bootstraps under heteroscedasticity because it uses a two-point distribution to generate errors [@mackinnon2017;@hapuhinna2021]. Other bootstraps, such as the parametric bootstrap, assume the error follows i.i.d with equal variance. Under these assumptions, the generated responses based on the resampled residuals are close to the true responses. However, when heteroscedasticity exists, the variance depends on the predictor, causing the generated responses to differ from the true responses [@hapuhinna2021]. In contrast, the wild bootstrap uses a two-point distribution such that the generated residuals preserve the variances and the within-cluster covariances of the errors [@mackinnon2017]. 
<!-- ML: I tend to think of this as a procedure within residuals bootstrap, where you can have a subsection, and can say which part is different. Also, is the procedure below for MLM? Do they resample the u's and the e's separately? YZ: I don't think they sample u and e directly-->

## Cases Bootstrap 

Similar to the wild bootstrap, the cases bootstrap can overcome the problem of heteroscedasticity. The cases bootstrap has the least restrictive assumption among all bootstrapping procedures because it only assumes a correct specification of hierarchical structure. It resamples the observations (including predictors contained in $\bv{X}_j$ and $\bv{Z}_j$) with replacements [@davison1997]. There are multiple versions of cases bootstrap for multilevel data [@vanderleeden2008]. For any version, the cases bootstrap keeps each observed response $y_{ij}$ pairing with the observed predictors $\bv{X}_{ij}$ and $\bv{Z}_{ij}$. Note that the size of each bootstrap sample could be different from the original sample size when the clusters are unbalanced. One version of cases bootstrap is to only resample level-2 units and keeps level-1 units intact.

1. Draw a sample of size $J$, $\{j^*_k, k = 1,...,J\}$, with replacement from the identifier of level-2 units.
2. For each $k$, draw a sample of entire cases, ${(y_{ik}^*, \bv{X}_{ik}^*, \bv{Z}_{ik}^*)}$, where $i = 1,...,n_k^*$, with replacement from the selected level-2 unit $j = j^*_k$.
3. Fit the two-level model to the bootstrap data and obtain parameter estimates.
4. Repeat steps 1-3 $B$ times to obtain a bootstrap distribution of parameter estimates. 

<!-- 1. Draw a sample of size $J$, $\{j^*_k, k = 1,...,J\}$, with replacement from level-2 unit numbers.  -->
<!-- 2. For each $k$, draw a sample of entire cases with replacement from level-2 unit $j = j^*_k$ so that we have a set of data ${(y_{ik}^*, \bv{X}_{ik}^*, \bv{Z}_{ik}^*), i = 1,...,n_k^*}$. -->
<!-- 3. Refit the two-level model to the bootstrap data and obtain parameter estimates. -->
<!-- 4. Repeat steps 1-3 $B$ times and compute the bias-corrected estimates and standard errors.  -->

An alternative procedure is to first resample the level-2 units and then resample level-1 units within each cluster. 

1. Draw one entire level-2 unit, ${(\bv{y}_{j}, \bv{X}_{j}, \bv{Z}_{j})}$, with replacement.
2. Draw a sample of level-1 observations with size $n_j$ from the selected level-2 unit.
3. Repeat steps 1-2 $J$ times.
4. Fit the two-level model to bootstrap data and obtain parameter estimates.
5. Repeat steps 1-4 $B$ times to obtain a bootstrap distribution of parameter estimates. 

Selecting the appropriate cases bootstrap procedure depends on the degree of randomness of the sampling and the sample size for both levels [@vanderleeden2008]. For example, if the repeated measures from individuals are collected, then it is appropriate to resample only individuals (level-2). However, for another example, if individuals' responses from multiple countries are collected, researchers could consider first resampling the countries with replacements, then resampling the individuals within selected countries with replacements. Compared to other bootstrap algorithms, cases bootstrap is less efficient but provides consistent estimators when the normality or homoscedasticity assumptions are violated [@vanderleeden2008]. 
<!-- ML: do you mean resampling individuals within counties? Meaning that the counties are fixed (which is inconsistent with MLM)? And do you mean countries? -->
