Because multilevel data involve units at multiple levels, bootstrapping is more complex as it can involve resampling at different levels. For a simple example of $N$ students nested with $J$ schools, there are at least three resampling schemes: (a) resample only schools, (b) resample only students, and (c) resample schools first, and then students. An additional concern, especially for cases bootstrap, is that the students do not constitute independent observations, as they are clustered within schools. As a basic principle, our bootstrap scheme should honor the nesting structure of the data. Therefore, we should not directly resample students in our example, as that would break the nesting structure such that students from the same school may end up in different schools in the bootstrapped sample.

As a result, several bootstrap algorithms have been proposed to account for the nesting structure, which will be further discussed below. As an overview, the parametric and the residual bootstrap preserve the clustered structure by simulating new error terms ($e_{ij}$) and random effects (the $u_j$ terms), because they are presumed independent in the statistical model for MLM. These two methods are thus *model-based* because they require specifying a model to obtain quantities that are assumed independent (i.e., the $e$'s and the $u$'s). Each new bootstrap sample is formed by generating a new sample of only the response variable ($Y$), using the deterministic component of the model (e.g., $X \gamma$, the fixed effect component in equation X) and the simulated errors and random effects. The parametric and the residual bootstrap differ in terms of whether the errors/random effects are sampled from an assumed parametric distribution (e.g., normal) or the empirical distribution of the model estimates. On the other hand, the cases bootstrap starts by resampling the top level of observations, which are assumed independent. While one version of the cases bootstrap would resample only the top level, another version would further proceed to resample the units with a cluster, and repeat for every cluster, as long as the observations are assumed independent and exchangeable within each cluster. 
<!-- YZ: Just a note: we should use consistent symbols throughout manuscript -->

We should also point out that the parametric and the residual bootstrap only simulate new $Y$, whereas the cases bootstrap gives new samples of all variables in the data, including the predictors $X$. Because of that, the formers assume fixed $X$ across repetitions, making it more suitable for designs where the predictors are fixed (e.g., randomized experiments with a fixed manipulated variable). On the other hand, the cases bootstrap allows the predictors to vary across the bootstrap samples. In some situations, the inferences could be different when the predictors are considered fixed vs. random [see @gelman2007, section 11.4, for more discussion]. While we treat the predictors as fixed in this chapter, one can specify random components for $X$, and adapt the parametric and the residual bootstrap to resample the random components of $X$. Similarly, one can introduce stratifying variables to cases bootstrap to make the bootstrap scheme resemble fixed predictors.
<!-- YZ: Can we add references here so that if readers are interested in these directions, they can refer to these references-->