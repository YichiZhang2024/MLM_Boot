Because multilevel data involve units at multiple levels, bootstrapping is more complex because it can potentially involve resampling at different levels. For example, for a simple example of $N$ students nested with $J$ schools, one can resample (a) schools, (b) students, and (c) schools first, and then students. An additional concern, especially for cases sampling, is that the students do not constitute independent observations, as they are clustered within schools. As a basic principle, our bootstrap scheme should honor the nesting structure of the data. Therefore, we should not directly resample students in our example, as that would break the nesting structure such that students from the same school may end up in different schools in the bootstrapped sample.

As a result, several bootstrap algorithms have been proposed to account for the nesting structure, which will be further discussed below. As an overview, the parametric and the residual bootstrap preserve the independent structure by simulating new error terms ($e_{ij}$) and random effects (the $u_j$ terms), because they are presumed independent in the statistical model. Therefore, these methods are model-based because they require specifying a model to obtain quantities that are assumed independent (i.e., the $e$'s and the $u$'s). Each new bootstrap sample is formed by generating a new sample of only the response variable ($Y$), using the deterministic component of the model (e.g., $X \gamma$, the fixed effect component in equation X) and the simulated errors and random effects. The parametric and the residual bootstrap differ in terms of whether the errors/random effects are sampled from an assumed parametric distribution (e.g., normal) or the empirical distribution of the model estimates. The cases bootstrap, on the other hand, starts by resampling the top level of observations, which are assumed independent. While one version of the cases bootstrap would resample only the top level, another version would further proceed to resample the units with a cluster, and repeat for every cluster, as long as the observations are assumed independent and exchangeable within each cluster. 

One additional thing to point out is that because the parametric and the residual bootstrap only simulates new $Y$, whereas the cases bootstrap gives new samples of all variables in the data, including the predictors $X$. Because of that, the formers assume fixed $X$ across repetitions, which may make it more suitable for designs where the predictors are fixed (e.g., randomized experiments with a fixed manipulated variable). On the other hand, the cases bootstrap allows the predictors to vary across the bootstrap samples. In some situations the inferences could be different when the predictors are considered fixed vs. random [see @gelman2007, section 11.4, for more discussion]. While it's beyond the scope of the chapter, one can adapt the parametric and the residual bootstrap by specifying a random component for $X$, so that it can be resampled to represent random predictors; one can similar introduce stratifying variables to cases bootstrap to make the bootstrap scheme resemble fixed predictors.
