Because multilevel data involve units at multiple levels, bootstrapping is more complex as it can involve resampling at different levels. For a simple example of $N$ students nested with $J$ schools, there are at least three resampling schemes: (a) resample only schools, (b) resample only students, and (c) resample schools first, and then students. An additional concern, especially for cases bootstrap (also known as the cluster/block/pairs bootstrap), is that the students do not constitute independent observations, as they are clustered within schools. As a basic principle, the bootstrap method should honor the nesting structure of the data. Therefore, we should not directly resample students in our example, as that would break the nesting structure such that students from the same school may end up in different schools in the bootstrap sample.

As a result, several bootstrap algorithms have been proposed to account for nesting structures, including parametric, residual, wild, and cases bootstrap methods [@goldstein2011; @vanderleeden2008; @modugno2015; @davison1997], which will be further discussed below. In general, the parametric, residual, and wild bootstrap preserve the clustered structure by simulating new error terms ($\boldsymbol{\epsilon}_{j}$) and random effects (the $\mathbf{U}_j$ terms), because they are presumed independent in the statistical model for MLM. These three methods are thus *model-based* because they require specifying a model to obtain quantities that are assumed independent (i.e., the $\boldsymbol{\epsilon}$'s and the $\mathbf{U}$'s). Each new bootstrap sample is formed by generating a new sample of only the response variable ($Y$), using the deterministic component of the model (e.g., $\bv X \bv \beta$, the fixed effect component in equation\ \@ref(eq:mod-eq) and the simulated errors and random effects. The parametric and the residual bootstrap differ in terms of whether the errors/random effects are sampled from an assumed parametric distribution (e.g., normal) or the empirical distribution of the model estimates. The wild bootstrap further relaxes the homoscedasticity assumption. On the other hand, the cases bootstrap starts by resampling the top level of observations, which are assumed independent. While one version of the cases bootstrap resamples only the top level, another version further resamples the units with a cluster, and repeats for every cluster, as long as the observations are assumed independent and exchangeable within each cluster. 

We should also point out that the parametric, residual, and wild bootstrap only simulate new $Y$, whereas the cases bootstrap gives new samples of all variables in the data, including the predictors $X$. Because of this, the former methods assume fixed $X$ across repetitions, making it more suitable for designs where the predictors are fixed (e.g., randomized experiments with a fixed manipulated variable). On the other hand, the cases bootstrap allows the predictors to vary across the bootstrap samples. In some situations, the inferences could be different when the predictors are considered to be either fixed or random [see @gelman2007, section 11.4, for more discussion]. While we treat the predictors as fixed in this chapter, one can specify random components for $X$, and adapt the parametric and the residual bootstrap to resample the random components of $X$. Similarly, one can introduce stratifying variables to cases bootstrap to make the bootstrap scheme resemble fixed predictors. Detailed discussion of each of these four bootstrap methods follows below. 
<!-- YZ: Can we add references here so that if readers are interested in these directions, they can refer to these references-->