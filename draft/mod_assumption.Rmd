This chapter focuses on two-level models, where participants or observations (level-1 units) are nested within clusters (level-2 units). For $J$ clusters of $n_1, n_2, \dots, n_j$ participants, and $p-1$ predictors, a two-level model is given by
<!-- ML: No need to bold the subscripts; e.g., \bv y_j or \bv{y}_j WT: Thanks!-->
\begin{equation}
\bv{y}_j = \bv{X}_j\bv{\beta} + \bv{Z}_j \bv{u}_j + \bv{\epsilon}_j
(\#eq:mod-eq)
\end{equation}
where $\bv{y}_j$ is an $n_j \times 1$ outcome vector for the $j$th cluster, $\bv{X}_j$ is an $n_j \times p$ design matrix for the unknown fixed parameters (including the intercept and predictors), $\bv{\beta}$ is an $p \times 1$ vector for the fixed effects, $\bv{Z}_j$ is an $n_j \times r$ design matrix for $r$ random effects, $\bv{u}_j$ is an $r \times 1$ vector of the unknown random components (including the random intercept and random slopes), <!-- ML: is there a reason to use capital U instead of small u? I generally use uppercase letter for a matrix, like X or Z. WT: makes sense. I was following Hox but I think lower case u for a vector is clearer. --> and $\bv{\epsilon}_j$ is an $n_j \times 1$ error vector. $\bv{u}_j$ and $\bv{\epsilon}_j$ are assumed to follow a multivariate normal distribution:
<!-- YZ: Maybe add a sentence what r is WT: I just changed to this sentence "$\bv{Z_j}$ is a $n_j \times r$ design matrix for $r$ random effects". Does it sound clear enough?-->
\begin{equation}
\begin{pmatrix}
\bv{\epsilon}_j \\ \bv{U}_j
\end{pmatrix}
\sim
\mathcal{N}
\left[
\begin{pmatrix}
\bv{0} \\ \bv{0}
\end{pmatrix}, 
\begin{pmatrix}
\bv{\Sigma} & \bv{0} \\
\bv{0} & \bv{T}
\end{pmatrix}
\right]
\end{equation}
where $\bv{T}$ is the variance-covariance matrix of the random components, and assuming homoscedasticity, $\bv{\Sigma} = \sigma^2 \bv I$ for $\sigma^2$ being the error variance. <!-- e_j is a vector of length n_j, so its covariance should be a n_j x n_j matrix, not a scalar. \sigma^2 is a scalar. WT: does it look right now? -->

Model\ \@ref(eq:mod-eq) implies five main assumptions: linearity, independence of errors, homoscedasticity, normality, and correct model specification. First, the model assumes the relationship between the predictors and outcome is linear. This assumption can be relaxed with a non-linear multilevel model [e.g., @davidian2017; @lindstrom1990]. <!-- ML: linearity implies more than monotonicity. I don't think you need to go too deep into it, but be accurate. --> Second, the independence of errors assumption denotes that the level-2 random components, $\bv{u}_j$, are independent between clusters and of the level-1 errors, $\bv{\epsilon}_j$ [@snijders2012]. 

 <!-- ML: the one you talk about here is more about exogeneity, which is more about specification and different for independent observations (e.g., no correlation in the T and \Sigma matrix for elements from different observations/clusters). Not sure which one you intend to say, but be clear. --> 
Third, the homoscedasticity assumption requires equal variances of errors and random effects across all values of the predictors. <!-- ML: It is not just the error, but also the u's as well --> Heteroscedasticity, meaning unequal variances, happens when the total variance of $\bv{y}_j$, given the values of $\bv{X}_j = \bv{x}_j$, depends on  $\bv{x}_j$. For example, in a model with age as a predictor, heteroscedasticity occurs when the error variance is different for participants of different ages. Violation of homoscedasticity can result in biased estimation of the fixed effects and their standard errors, and hence erroneous statistical inferences [@raudenbush2002; @snijders2012; @huang2022; @zhang2022]. 
<!-- YZ: Heteroscedasticity in mlm can be at two levels. I think it'll be helpful to mention MLM also requires equal variances of random effects.  WT: thanks for the reminder! Does it look better now or should I edit more?-->
Fourth, the error and random components are assumed to be multivariate normal, which may be violated with skewed distributions and the presence of outliers or influential observations. <!-- ML: outliers are not the only reason for non-normality; some data are just not normally distributed without outliers --> <!-- ML: I don't think this is the case, if it is just non-normal; the central limit theorem says things should still be okay. Some simulation results can be found in Maas & Hox 2004, etc --> Finally, the estimates of model parameters are meaningful only if the model is correctly specified. The correct specification assumption requires that the model includes relevant predictors, cluster means of participant-level predictors, and relevant random slope terms [@snijders2012]. <!-- ML: citation? -->
