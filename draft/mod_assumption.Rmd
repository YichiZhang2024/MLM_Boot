This chapter focuses on two-level models, where participants are nested within clusters. For $J$ clusters of $n_1, n_2, \dots, n_j$ participants, and $p-1$ predictors, a two-level model is given by
<!-- ML: No need to bold the subscripts; e.g., \bv y_j or \bv{y}_j -->
\begin{equation}
\bv{y_j} = \bv{X_j}\bv{\beta} + \bv{Z_j} \bv{U_j} + \bv{\epsilon}_j
(\#eq:mod-eq)
\end{equation}
where $\bv{y_j}$ is a $n_j \times 1$ outcome vector for the $j$th cluster, $\bv{X_j}$ is a $n_j \times p$ design matrix for the unknown fixed parameters (including the intercept and predictors), $\bv{\beta}$ is a $p \times 1$ vector for the fixed effects, $\bv{Z_j}$ is a $n_j \times r$ design matrix for the random effects, $\bv{U_j}$ is a $r \times 1$ vector of the unknown random components (including the random intercept and random slopes), <!-- ML: is there a reason to use capital U instead of small u? I generally use uppercase letter for a matrix, like X or Z --> and $\bv{e_j}$ is a $n_j \times 1$  error vector. $\bv{U_j}$ and $\bv{e_j}$ are assumed to follow a multivariate normal distribution:
<!-- YZ: Maybe add a sentence what r is -->
\begin{equation}
\begin{pmatrix}
\bv{\epsilon_j} \\ \bv{U_j}
\end{pmatrix}
\sim
\mathcal{N}
\left[
\begin{pmatrix}
\bv{\rlap{0}/} \\ \bv{\rlap{0}/}
\end{pmatrix}, 
\begin{pmatrix}
\bv{\Sigma} & \bv{\rlap{0}/} \\
\bv{\rlap{0}/} & \bv{T}
\end{pmatrix}
\right]
\end{equation}
where $\bv{T}$ is the variance-covariance matrix of the random components, and assuming homoscedasticity, $\bv{\Sigma} = \sigma^2 \bv I$ is a scalar for $\sigma^2$ being the error variance. <!-- e_j is a vector of length n_j, so its covariance should be a n_j x n_j matrix, not a scalar. \sigma^2 is a scalar -->. 

Model\ \@ref(eq:mod-eq) implies five main assumptions: linearity, independence of errors, homoscedasticity, normality, and correct model specification. First, the model assumes the relationship between the predictors and outcome is linear. That is, the relationship between $\bv{X_j}$ and $\bv{y_j}$ is monotonically increasing, decreasing, or zero. This assumption can be relaxed with a non-linear multilevel model. <!-- ML: linearity implies more than monotonicity. I don't think you need to go too deep into it, but be accurate. -->

Second, the independence of errors assumption denotes that the errors are uncorrelated with the predictors at the cluster-level, i.e. $Cor(\bv{X_j}, \bv{e_j})=\bv{\rlap{0}/}$. <!-- ML: the one you talk about here is more about exogeneity, which is more about specification and different for independent observations (e.g., no correlation in the T and \Sigma matrix for elements from different observations/clusters). Not sure which one you intend to say, but be clear. --> Moreover, the homoscedasticity assumption requires that the error variances are equal across all values of the predictors. <!-- ML: It is not just the error, but also the u's as well --> Heteroscedasticity, meaning unequal error variances, happens when the error variance of $\bv{y_j}$, given the values of $\bv{X_j} = \bv{x_j}$, depends $\bv{x_j}$. For example, in a model with age as a predictor, heteroscedasticity occurs when the error variance is different for participants of different ages. Violation of homoscedasticity can result in biased estimation of the fixed effects and their standard errors, and hence erroneous statistical inferences [add citations]. 
<!-- YZ: Heteroscedasticity in mlm can be at two levels. I think it'll be helpful to mention MLM also requires equal variances of random effects.  -->
Fourth, the error and random components are assumed to be multivariate normal. The presence of outliers or influential observations and skewness of the distributions can lead to the violation of the multivariate normality assumption. <!-- ML: outliers are not the only reason for non-normality; some data are just not normally distributed without outliers --> If this assumption fails, the standard errors of the fixed effects are biased, resulting in inflated Type I error rates [add citations]. <!-- ML: I don't think this is the case, if it is just non-normal; the central limit theorem says things should still be okay. Some simulation results can be found in Maas & Hox 2004, etc --> Finally, the estimates of model parameters are meaningful only if the model is correctly specified. The correct specification assumption requires that the model includes relevant predictors, cluster means of participant-level predictors, and relevant random slope terms. <!-- ML: citation? -->