Multilevel data is common in social and behavioral science research. For example, students are nested within classrooms or observations of the same individual assessed at different time points. To account for the clustered nature of data, multilevel models (MLM) or hiearchical linear models (HLM) has been widely used. The major estimation methods in MLM is the maximum likelihood (ML) estimation, which generates the most probable value of model parameters given the observed data [@raudenbush2002]. The common versions of ML are restricted maximum likelihood (REML) and full information maximum likelihood (FIML). The accuracy of the ML estimates and associated statistical inferences depend on whether the underlying assumptions are satisfied. Specifically, ML assumes a sufficiently large sample size; residuals follow independent and identical distributions (i.i.d.) [@leeden2008]. When these two assumptions are violated, the ideal properties of ML, such as consistenciy, efficiency, and asymptotic normality, might not hold any more and leading to biased standard error estimates with inaccurate inferences. Bootstrap, a resampling method that approximates the distribution of statistics, can be used to obtain more accurate standard error estimates under this situation [@busing1993]. 

The goal of this chapter is to explain the bootstrapping procedures for MLM and illustrate how they can be used in applied research. We start with an overview of MLM with a focus on assumption violations, and the general idea of bootstrap methods for regression models. Next, we provide an extensive discussion on bootstrap methods for MLM, including challenges and different types of bootstrapping procedures. Next, we transit to introduce bootstrap confidence intervals and comparisons among available options. The last section provides an illustrative example that demonstrates the usage of various bootstrap methods and comparisons of results. Them, this chapter ends with suggestions on available software and resources for MLM bootstrap methods. 