Bootstrap methods are useful to compute standard errors, confidence intervals, and testing hypotheses for an estimator when some model assumptions are violated [@efron1993; @davison1997; @wilcox2017a]. The general idea of bootstrapping is to create many samples by repeatedly drawing observations from a dataset and computing the estimator for each simulated sample. The bootstrap process yields an approximate distribution of the estimator, from which the standard errors and confidence intervals can be calculated. The bootstrap methods have been shown to perform well in controlling the Type I error rates and maintaining accurate confidence interval coverages in situations such as when the normality assumption is violated [e.g., @efron1988; @strube1988] and the homoscedasticity assumption fails [e.g., @wilcox2001; @krishnamoorthy2007]. As will be discussed in the following sections, there are many bootstrap procedures, some of which better control the Type I error rates than the others in certain situations. We begin with an introduction of one of the most basic bootstrap methods---bootstrap-t, also known as percentile-t---for computing confidence interval and standard error for the sample mean.

Suppose that we collected data with $n$ observations, $X_1, \dots, X_n$, and aim to test against the null hypothesis, $H_0: \mu=\mu_0$, that the population mean is zero. Recall that the student *t-*statistic is 
\begin{equation}
T = \frac{\bar X - \mu_0}{se}, 
\end{equation}
where $\bar X = \frac{1}{n}\sum_i^n X_i$ is the sample mean, $\mu_0$ is the null value of the mean, $se=s/\sqrt{n}$ is the standard error of the mean, and $s$ is the sample standard deviation. If normality holds, $T$ has a symmetric distribution, and the 95% confidence interval for $\bar X$ is 
\begin{equation}
[\bar X-t_{1-\alpha/2} se, \bar X - t_{\alpha/2} se],  
\end{equation}
where $t_{1-\alpha/2}$ and $t_{\alpha/2}$ are the $1-\alpha/2$ and $t_{\alpha/2}$ quantiles of $T$. 

However, our data has a skewed distribution and violates the normality assumption. To account for nonnormality, we perform bootstrapping in the following procedure [@wilcox2017a; @efron1993]:

1. Randomly draw $n$ observations from the observed data, *with replacement*, to create a bootstrap sample, $X^*_1, \dots, X_n^*$. 
2. After gathering a bootstrap sample, compute the *bootstrap sample mean*, $\bar X^*_b = \frac{1}{n}\sum_i^n X^*_i$, the bootstrap sample standard deviation, $s^*_b=\sqrt{\frac{\sum_i^n(X^*_i - \bar X^*)^2}{n}}$, the bootstrap standard error $se^*_b = \frac{s^*}{\sqrt{n}}$, as well as the bootstrap *t-*statistic, $T^*_b=\frac{\bar X^*-\mu_0}{s^*/\sqrt{n}}$. 
3. Repeat steps (1) and (2) $B$ times to obtain $B$ bootstrap *t*-statistics, $T^*_1, \dots, T^*_B$, which forms a distribution that approximates the sampling distribution of the *t*-statistic. 
4. Sort the bootstrap sample means in ascending order to get $T^*_{(1)} \leq T^*_{(2)} \leq \dots \leq T^*_{(B)}$. 

Based on the bootstrap samples, we can estimate the sample mean by taking the average of the bootstrap sample means, $\bar X^*=\frac{1}{B}\sum_b^BX^*_b$, and the standard error of the mean by taking the average of the bootstrap standard errors, $se^*=\frac{1}{B}\sum_b^Bse^*_b$. An approximate 95% confidence interval for the sample mean is
\begin{equation}
[\bar X^* - t_{1-\alpha/2}^{*} se^{*}, \bar X^* - t_{\alpha/2}^{*} se^{*}]
\end{equation}
where $t^*_{1-\alpha/2}$ and $t^*_{1-\alpha/2}$ denote the the $1 - \alpha/2$ and $\alpha/2$ quantiles of the bootstrap distribution. For example, if $B=1000$ with at the significance level of $\alpha=.05$, $t^*_{1-\alpha/2}= t^*_{.025}$ and $t^*_{\alpha/2} = t^*_{.975}$.
