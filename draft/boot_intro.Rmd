Bootstrap methods are useful to compute standard errors, confidence intervals, and testing hypotheses for an estimator without assuming normality [@efron1993; @davison1997; @wilcox2017a]. The general idea of bootstrapping is to create many samples by repeatedly drawing observations from a dataset and compute the estimator for each simulated samples. The bootstrap process yields an approximate distribution of the estimator, from which the standard errors and confidence intervals can be calculated. The bootstrap methods have been shown to perform well in controlling the Type I error rates and maintaining accurate confidence interval coverages in situations such as when the normality assumption fails [e.g., @efron1988; @strube1988] and the equal variance assumption fails [e.g., @wilcox2001; @krishnamoorthy2007]. As will be discussed in length in the following sections, there are many bootstrap procedures, some better control the Type I error rate than the others. We begin with an introduction of one of the most basic bootstrap methods---bootstrap-t, also known as percentile-t. 

Suppose that we collected data with $n$ observations, $X_1, \dots, X_n$, and aim to testing against the null hypothesis, $H_0: \mu=\mu_0$, that the population mean is zero. Recall that the student *t-*statistic is 
\begin{equation}
T = \frac{\bar X - \mu_0}{se}, 
\end{equation}
where $\bar X = \frac{1}{n}\sum_i^n X_i$ is the sample mean, $\mu_0$ is the null value of the mean, $s$ is the sample standard deviation, and $se=s/\sqrt{n}$ is the standard error of the mean. If normality holds, $T$ has a symmetric distribution, and the 95% confidence interval for $\bar X$ is 
\begin{equation}
[\bar X-t_{u} se, \bar X - t_{\ell} se],  
\end{equation}
where $t_\ell{}$ and $t_u$ are the .025 and .975 quantiles of $T$. 

However, our data has a skewed distribution and violates the normality assumption. To account for nonnormality, we perform bootstrapping in the following procedure [@wilcox2017a; @efron1993]:

1. Randomly draw $n$ observations from the observed data, *with replacement*, to create a bootstrap sample, $X^*_1, \dots, X_n^*$. 
2. After gathering a bootstrap sample, compute the *bootstrap sample mean*, $\bar X^*_b = \frac{1}{n}\sum_i^n X^*_i$, the bootstrap sample standard devaition, $s^*_b=\sqrt{\frac{\sum_i^n(X^*_i - \bar X^*)^2}{n}}$, the bootstrap standard error $se^*_b = \frac{s^*}{\sqrt{n}}$, as well as the bootstrap *t-*statistic, $T^*_b=\frac{\bar X^*-\mu_0}{s^*/\sqrt{n}}$. 
3. Repeat step (1) and (2) $B$ times to obtain $B$ bootstrap *t*-statistics, $T^*_1, \dots, T^*_B$, which forms a distribution that approximates the sampling distribution of the *t*-statistic. 
4. Sort the bootstrap sample means in ascending order to get $T^*_{(1)} \leq T^*_{(2)} \leq \dots \leq T^*_{(B)}$. 

Based on the bootstrap samples, we can estiamte the sample mean by taking the average of the bootstrap sample means, $\bar X^*=\frac{1}{B}\sum_b^BX^*_b$, and the standard error of the mean by taking the average of the bootstrap standard errors, $se^*=\frac{1}{B}\sum_b^Bse^*_b$. An apprxomiate 95% confidence interval for the sample mean is
\begin{equation}
[\bar X^* - T^*_{(u)}se^*, \bar X^* - T^*_{(\ell)}se^*]
\end{equation}
where $T^*_{(u)}$ and $T^*_{(\ell)}$ denote .025 and .975 quantiles of $T^*$, respectively. For example, if $B=1000$, $T^*_{(u)}= T_{(975)}$ and $T^*_{(\ell)} = T_{(25)}$.
